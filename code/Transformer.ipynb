{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "025adfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Transformer\n",
    "\n",
    "chars = \" ^$#%()+-./0123456789=@ABCDEFGHIKLMNOPRSTVXYZ[\\\\]abcdefgilmnoprstuy\"\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "VOCAB_SIZE=len(chars)\n",
    "PAD_IDX, BOS_IDX, EOS_IDX = 0,1,2\n",
    "\n",
    "file = open(\"/home/arvid/data/USTPO_paper_5x/USTPO_5x_parsed.pickle\",'rb')\n",
    "data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d438c3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 23 14:44:30 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM3...  On   | 00000000:1E:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    66W / 350W |   4066MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM3...  On   | 00000000:23:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    49W / 350W |      5MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM3...  On   | 00000000:28:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    51W / 350W |      5MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM3...  On   | 00000000:2D:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    50W / 350W |      5MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM3...  On   | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    49W / 350W |      5MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM3...  On   | 00000000:42:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    46W / 350W |      5MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM3...  On   | 00000000:4C:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    68W / 350W |   8125MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM3...  On   | 00000000:4D:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    46W / 350W |      5MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   8  Tesla V100-SXM3...  On   | 00000000:8C:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    50W / 350W |      5MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   9  Tesla V100-SXM3...  On   | 00000000:91:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    47W / 350W |      5MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|  10  Tesla V100-SXM3...  On   | 00000000:96:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    91W / 350W |  32140MiB / 32510MiB |     80%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|  11  Tesla V100-SXM3...  On   | 00000000:9B:00.0 Off |                    0 |\n",
      "| N/A   42C    P0   135W / 350W |  32133MiB / 32510MiB |     44%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|  12  Tesla V100-SXM3...  On   | 00000000:B5:00.0 Off |                    0 |\n",
      "| N/A   44C    P0   117W / 350W |  31755MiB / 32510MiB |     59%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|  13  Tesla V100-SXM3...  On   | 00000000:B6:00.0 Off |                    0 |\n",
      "| N/A   37C    P0   120W / 350W |  31755MiB / 32510MiB |     48%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|  14  Tesla V100-SXM3...  On   | 00000000:C0:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    48W / 350W |      5MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|  15  Tesla V100-SXM3...  On   | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    50W / 350W |      5MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "565bf17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device 8/16, name: Tesla V100-SXM3-32GB\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:8' if torch.cuda.is_available() else 'cpu')\n",
    "torch.torch.cuda.set_device(DEVICE)\n",
    "print(\"Using device \" + str(torch.torch.cuda.current_device()) + \"/\" + str(torch.cuda.device_count())\n",
    "      +\", name: \" + str(torch.cuda.get_device_name(0)))\n",
    "#device = torch.torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fcc2b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class ReactionDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, split, maxlen=160, rep=\" ^$#%()+-./0123456789=@ABCDEFGHIKLMNOPRSTVXYZ[\\\\]abcdefgilmnoprstuy\"):\n",
    "        self.split = split\n",
    "        self.data = data[self.split]\n",
    "        self.maxlen = maxlen\n",
    "        self.rep = rep\n",
    "        self.char_to_ix = { ch:i for i,ch in enumerate(rep) }\n",
    "        self.ix_to_char = { i:ch for i,ch in enumerate(rep) }\n",
    "        # Add augmentation methods here later\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        rs_smiles = self.data[index][\"rs\"]\n",
    "        ps_smiles = self.data[index][\"ps\"]\n",
    "        \n",
    "        \n",
    "        rs_smiles = self.ix_to_char[BOS_IDX] + rs_smiles + self.ix_to_char[EOS_IDX] + (self.maxlen-len(rs_smiles))*\" \"\n",
    "        ps_smiles = ps_smiles + (self.maxlen-len(ps_smiles)+2)*\" \"\n",
    "        \n",
    "        # Augment smiles here for train\n",
    "        \n",
    "        rs = torch.tensor([self.char_to_ix[char] for char in rs_smiles])\n",
    "        ps = torch.tensor([self.char_to_ix[char] for char in ps_smiles])\n",
    "        \n",
    "        return {\n",
    "            \"rs\": rs.to(dtype=torch.int64),\n",
    "            \"ps\": ps.to(dtype=torch.int64)\n",
    "            #'rs': F.one_hot(rs.to(dtype=torch.int64), num_classes=len(self.rep)),\n",
    "            #'ps':  F.one_hot(ps.to(dtype=torch.int64), num_classes=len(self.rep))\n",
    "        }\n",
    "\n",
    "datasets = {}\n",
    "dataloaders = {}\n",
    "for split in ['train', 'eval']:\n",
    "    datasets[split] = ReactionDataset(data=data,\n",
    "                                   split=split)\n",
    "\n",
    "    dataloaders[split] = DataLoader(datasets[split],\n",
    "                                    batch_size=32,\n",
    "                                    shuffle=(split != 'test'),\n",
    "                                    num_workers=4,\n",
    "                                    pin_memory=False)# Was True before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bec43e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 162])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#datasets[\"train\"].__getitem__(10)[\"rs\"]\n",
    "\n",
    "next(iter(dataloaders[\"train\"]))[\"rs\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7915645b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6979e301c0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "        mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX)\n",
    "    tgt_padding_mask[tgt == EOS_IDX] = True\n",
    "    return src_mask, tgt_mask, src_padding_mask.permute(1,0), tgt_padding_mask.permute(1,0)\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 200):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        token_embedding = token_embedding\n",
    "        return self.dropout((token_embedding + self.pos_embedding[:token_embedding.size(0), :]))\n",
    "    \n",
    "    \n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                    N_ENCODERS: int=6,\n",
    "                    N_DECODERS: int=6,\n",
    "                    EMBEDDING_SIZE: int=512,\n",
    "                    N_HEADS: int=8,\n",
    "                    SRC_VOCAB_SIZE: int=VOCAB_SIZE,\n",
    "                    TGT_VOCAB_SIZE: int=VOCAB_SIZE,\n",
    "                    DIM_FF: int=512,\n",
    "                    DROPOUT: float=0.0):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.transformer = Transformer(d_model=EMBEDDING_SIZE,\n",
    "                                       nhead=N_HEADS,\n",
    "                                       num_encoder_layers=N_ENCODERS,\n",
    "                                       num_decoder_layers=N_DECODERS,\n",
    "                                       dim_feedforward=DIM_FF,\n",
    "                                       dropout=DROPOUT)\n",
    "        \n",
    "        self.generator = nn.Linear(EMBEDDING_SIZE, TGT_VOCAB_SIZE)\n",
    "        \n",
    "        self.src_tok_emb = TokenEmbedding(SRC_VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "        self.tgt_tok_emb = TokenEmbedding(TGT_VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            EMBEDDING_SIZE, dropout=DROPOUT)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                tgt: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)\n",
    "    \n",
    "def loss_fn(logits, tgt, tgt_padding_mask):\n",
    "    #Shift outs and labels one step\n",
    "    logits = logits[:-1,:,:].permute(1,0,2)\n",
    "    one_hot_targets = F.one_hot(tgt, num_classes=VOCAB_SIZE)[1:,:,:].permute(1,0,2)\n",
    "    weights = (~tgt_padding_mask[:,:-1]).long().unsqueeze(-1).tile([1,1,TGT_VOCAB_SIZE])\n",
    "    \n",
    "    criterion = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    #print(torch.sum(torch.mean(criterion(logits, one_hot_targets.float())*weights,axis=0),axis=(0,1)))\n",
    "    loss = torch.sum(torch.mean(criterion(logits, one_hot_targets.float())*weights,axis=0),axis=(0,1))\n",
    "    return loss\n",
    "\n",
    "\n",
    "class CosineAnnealingWarmup():\n",
    "\n",
    "    def __init__(self,\n",
    "                 max_lr : float = 1e-3,\n",
    "                 min_lr : float = 1e-4,\n",
    "                 warmup_steps : int = 10,\n",
    "                 base_lr: float = 1e-4,\n",
    "                 gamma : float = 1,\n",
    "                 last_epoch : int = 200\n",
    "        ):\n",
    "        \n",
    "        self.base_max_lr = max_lr # first max learning rate\n",
    "        self.max_lr = max_lr # max learning rate in the current cycle\n",
    "        self.min_lr = min_lr # min learning rate\n",
    "        self.warmup_steps = warmup_steps # warmup step size\n",
    "        self.gamma = gamma # decrease rate of max learning rate by cycle\n",
    "        self.base_lr = base_lr\n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_lr(self, step):\n",
    "        if step <= self.warmup_steps:\n",
    "            return (self.max_lr - self.base_lr)*step / self.warmup_steps + self.base_lr\n",
    "        elif step <= 100:\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) \\\n",
    "                    * (1+ math.cos(math.pi * (step - self.warmup_steps)/(step*1.5- self.warmup_steps))) / 2\n",
    "\n",
    "cos = CosineAnnealingWarmup()\n",
    "plt.plot([cos.get_lr(i) for i in range(200)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c451bf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, split):\n",
    "    if split == \"train\": model.train()\n",
    "    else: model.eval()\n",
    "        \n",
    "    losses = 0\n",
    "    for batch in tqdm(dataloaders[split], colour='ffffff'):\n",
    "        src = batch[\"ps\"].to(DEVICE).permute(1,0)\n",
    "        tgt = batch[\"rs\"].to(DEVICE).permute(1,0)\n",
    "\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src=src, tgt=tgt)\n",
    "\n",
    "        logits = chemFormer(src=src,tgt=tgt,src_mask=src_mask,\n",
    "                            tgt_mask=tgt_mask,\n",
    "                            src_padding_mask=src_padding_mask,\n",
    "                            tgt_padding_mask=tgt_padding_mask,\n",
    "                            memory_key_padding_mask=src_padding_mask)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = loss_fn(logits, tgt, tgt_padding_mask)\n",
    "        if split==\"train\": loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        \n",
    "\n",
    "    return losses / len(dataloaders[split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ead988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_monitoring(model, optimizer, n_epochs, scheduler):\n",
    "    \n",
    "    model_dir = os.path.expanduser(\"~/models/transformers/\")\n",
    "    time()\n",
    "    from datetime import datetime\n",
    "    date_time = datetime.fromtimestamp(time())\n",
    "    model_str = str(date_time.year)+\"_\"+str(date_time.month)+\"_\"+str(date_time.day)+\"_\"+str(date_time.hour)+\"_\"+str(date_time.minute)+\"_\"+str(date_time.second)\n",
    "    os.mkdir(model_dir + model_str)\n",
    "    os.mkdir(model_dir + model_str + \"/logdir\")\n",
    "    print(\"tensorboard --logdir \" + model_dir + model_str + \"/logdir/ --port=6006\")\n",
    "    \n",
    "    params_dict = {\"N_ENCODERS\": N_ENCODERS,\n",
    "                   \"N_DECODERS\": N_DECODERS,\n",
    "                   \"EMBEDDING_SIZE\": EMBEDDING_SIZE,\n",
    "                   \"N_HEADS\": N_HEADS,\n",
    "                   \"SRC_VOCAB_SIZE\": SRC_VOCAB_SIZE,\n",
    "                   \"TGT_VOCAB_SIZE\": TGT_VOCAB_SIZE,\n",
    "                   \"DIM_FF\": DIM_FF,\n",
    "                   \"DROPOUT\": DROPOUT,\n",
    "                   \"BATCH_SIZE\": BATCH_SIZE,\n",
    "                   \"MAX_SEQ_LEN\": MAX_SEQ_LEN}\n",
    "    \n",
    "    with open(model_dir + model_str + '/params.pickle', 'wb') as handle:\n",
    "        pickle.dump(params_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    \n",
    "    best_loss = 999999999\n",
    "    running_losses = {'train': [], 'eval': []}    \n",
    "        \n",
    "    # Loop\n",
    "    writer = SummaryWriter(model_dir + model_str + \"/logdir/\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        lr = scheduler.get_lr(epoch)\n",
    "        print(lr)\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "            \n",
    "        losses = {}\n",
    "        print()\n",
    "        \n",
    "        for split in [\"train\", \"eval\"]:\n",
    "            print(\"Epoch \" + str(epoch) + \" \" + split + \" progression:\")\n",
    "            epoch_loss = train_epoch(model=model, optimizer=optimizer, split=split)\n",
    "            \n",
    "            losses[split] = epoch_loss\n",
    "            running_losses[split].append(epoch_loss)\n",
    "            writer.add_scalar('Loss/'+split, epoch_loss, epoch+1)\n",
    "            \n",
    "            if split == 'eval' and epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    best_model_wts = model.state_dict()\n",
    "                    torch.save(model.state_dict(),model_dir + model_str + \"/weights\")\n",
    "            \n",
    "        print(\"Epoch \" + str(epoch) + \" train/eval losses: \" + str(losses[\"train\"]) + \" / \" + str(losses[\"eval\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f5b3444",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ENCODERS=6\n",
    "N_DECODERS=6\n",
    "EMBEDDING_SIZE=512\n",
    "N_HEADS=8\n",
    "SRC_VOCAB_SIZE,TGT_VOCAB_SIZE=VOCAB_SIZE,VOCAB_SIZE\n",
    "DIM_FF=512\n",
    "DROPOUT=0.1\n",
    "BATCH_SIZE=32\n",
    "MAX_SEQ_LEN=160\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "chemFormer = Seq2SeqTransformer(N_ENCODERS=N_ENCODERS,\n",
    "                                N_DECODERS=N_DECODERS,\n",
    "                                EMBEDDING_SIZE=EMBEDDING_SIZE,\n",
    "                                N_HEADS=8,\n",
    "                                SRC_VOCAB_SIZE=SRC_VOCAB_SIZE,\n",
    "                                TGT_VOCAB_SIZE=TGT_VOCAB_SIZE,\n",
    "                                DIM_FF=DIM_FF,\n",
    "                                DROPOUT=DROPOUT).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(chemFormer.parameters(), lr=1e-5, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = CosineAnnealingWarmup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2d30ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_monitoring(model=chemFormer, optimizer=optimizer, n_epochs=200, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e86406e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff74da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    if src_mask != None: src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "    return ys[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d044f5d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CC(C)(C)OC(=O)N1CCC(N)CC1', 'O=C(O)c1c2ccccc2nn1C(c1ccccc1)(c1ccccc1)c1ccccc1']\n",
      "['CC(C)(C)OC(=O)NCCCCNC(=O)c1ccc(Nc2nc(NCc3ccc(OCCBr)cc3)nc(OCC(F)(F)F)n2)cc1']\n",
      "fail!\n",
      "\n",
      "\n",
      "['CC(C)CCCCCCCCCC(=O)OCCCCCCCCCCCCCCCCCC(=O)OC(C)(C)C']\n",
      "['CC(C)(C)OC(=O)N1CC=CC[C@H]1c1nc2cc(Br)sc2c(=O)[nH]1']\n",
      "fail!\n",
      "\n",
      "\n",
      "['COC(=O)c1ccc(B2OC(C)(C)C(C)(C)O2)cc1', 'N#Cc1ccc(Cl)cc1']\n",
      "['Oc1c(I)cc(I)cc1I', 'BrCCC(c1ccccc1)c1ccccc1']\n",
      "fail!\n",
      "\n",
      "\n",
      "Fail!\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [14:45:14] SMILES Parse Error: extra close parentheses while parsing: c1cccc(c1)C(c1ccccc1)(c1ccccc1)c1ccccc1)C(=O)O\n",
      "[14:45:14] SMILES Parse Error: extra close parentheses while parsing: c1cccc(c1)C(c1ccccc1)(c1ccccc1)c1ccccc1)C(=O)O\n",
      "RDKit ERROR: [14:45:14] SMILES Parse Error: Failed parsing SMILES 'c1cccc(c1)C(c1ccccc1)(c1ccccc1)c1ccccc1)C(=O)O' for input: 'c1cccc(c1)C(c1ccccc1)(c1ccccc1)c1ccccc1)C(=O)O'\n",
      "[14:45:14] SMILES Parse Error: Failed parsing SMILES 'c1cccc(c1)C(c1ccccc1)(c1ccccc1)c1ccccc1)C(=O)O' for input: 'c1cccc(c1)C(c1ccccc1)(c1ccccc1)c1ccccc1)C(=O)O'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CC(C)(C)OC(=O)N1CCN(c2ccc(C(=O)Nc3ccc(C(=O)O)cc3)cc2)CC1', 'CCN']\n",
      "['O[C@@H](CNc1ccc2ncc(-c3cc4ccccc4o3)n2n1)c1cccnc1']\n",
      "fail!\n",
      "\n",
      "\n",
      "['CCCCCCCCCCCCCCCCCC(=O)NC(OC(C)(C)C)OC(C)(C)C']\n",
      "['Nc1cccc(Br)n1', 'O=C=NCCCl']\n",
      "fail!\n",
      "\n",
      "\n",
      "['CC(C)(C)OC(=O)N1CCN(C(=O)c2cccc(C(F)(F)F)c2)CC1', 'NCCCN']\n",
      "['CS(=O)(=O)c1ccc(CNc2ccc(-c3c(N)nc(N)nc3C3CCCC3)cc2)cc1']\n",
      "fail!\n",
      "\n",
      "\n",
      "Fail!\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [14:45:19] Explicit valence for atom # 45 O, 3, is greater than permitted\n",
      "[14:45:19] Explicit valence for atom # 45 O, 3, is greater than permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CC(C)(C)OC(=O)c1ccc(C(c2ccc(Cl)cc2)N2CCN(C3CCN(C(OC(C)(C)C)C(=O)OC(C)(C)C)CC3)CC2)cc1']\n",
      "['O=C1CSc2cc(Br)ccc2N1', 'O=S(=O)(c1ccc(Cl)cc1)C(F)(F)F']\n",
      "fail!\n",
      "\n",
      "\n",
      "['COC(=O)C=C(c1ccccc1)c1ccc(C(=O)O)cc1', 'C1CNCCN1']\n",
      "['Brc1cccc2cccnc12', 'Oc1cccc2cccnc12']\n",
      "fail!\n",
      "\n",
      "\n",
      "['CC(C)(C)OC(=O)N1CCC(N2CCC2)C1', 'O=C(Cl)c1ccccc1']\n",
      "['CCC1OC(=C2C(=O)Nc3ccc(F)cc32)c2ncccc21']\n",
      "fail!\n",
      "\n",
      "\n",
      "Fail!\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [14:45:24] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 4\n",
      "[14:45:24] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 4\n",
      "\n",
      "RDKit ERROR: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CC(=O)Cl', 'CC(C)(C)OC(=O)N1CCNCC1']\n",
      "['COC(=O)Nc1ccc(-c2ccccc2C(F)(F)F)cc1']\n",
      "fail!\n",
      "\n",
      "\n",
      "Fail!\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [14:45:27] Can't kekulize mol.  Unkekulized atoms: 19 20 21 22 23\n",
      "[14:45:27] Can't kekulize mol.  Unkekulized atoms: 19 20 21 22 23\n",
      "\n",
      "RDKit ERROR: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail!\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [14:45:28] SMILES Parse Error: extra close parentheses while parsing: c1cc(ccc1)C(c1cccc(c1)C(=O)O)c1ccc(cc1)C(C(=O)O)OC)c1ccccc1\n",
      "[14:45:28] SMILES Parse Error: extra close parentheses while parsing: c1cc(ccc1)C(c1cccc(c1)C(=O)O)c1ccc(cc1)C(C(=O)O)OC)c1ccccc1\n",
      "RDKit ERROR: [14:45:28] SMILES Parse Error: Failed parsing SMILES 'c1cc(ccc1)C(c1cccc(c1)C(=O)O)c1ccc(cc1)C(C(=O)O)OC)c1ccccc1' for input: 'c1cc(ccc1)C(c1cccc(c1)C(=O)O)c1ccc(cc1)C(C(=O)O)OC)c1ccccc1'\n",
      "[14:45:28] SMILES Parse Error: Failed parsing SMILES 'c1cc(ccc1)C(c1cccc(c1)C(=O)O)c1ccc(cc1)C(C(=O)O)OC)c1ccccc1' for input: 'c1cc(ccc1)C(c1cccc(c1)C(=O)O)c1ccc(cc1)C(C(=O)O)OC)c1ccccc1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail!\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [14:45:29] SMILES Parse Error: extra close parentheses while parsing: c1ccccc1C(=O)Nc1ccccc1)Cl\n",
      "[14:45:29] SMILES Parse Error: extra close parentheses while parsing: c1ccccc1C(=O)Nc1ccccc1)Cl\n",
      "RDKit ERROR: [14:45:29] SMILES Parse Error: Failed parsing SMILES 'c1ccccc1C(=O)Nc1ccccc1)Cl' for input: 'c1ccccc1C(=O)Nc1ccccc1)Cl'\n",
      "[14:45:29] SMILES Parse Error: Failed parsing SMILES 'c1ccccc1C(=O)Nc1ccccc1)Cl' for input: 'c1ccccc1C(=O)Nc1ccccc1)Cl'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail!\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [14:45:31] SMILES Parse Error: extra close parentheses while parsing: CC(C)(C)OC(=O)N1CCC(CC1)C(=O)Nc1ccc(cc1)C(=O)Nc1ccc(cc1)C(F)(F)F)c1cccc(c1)C(F)(F)F\n",
      "[14:45:31] SMILES Parse Error: extra close parentheses while parsing: CC(C)(C)OC(=O)N1CCC(CC1)C(=O)Nc1ccc(cc1)C(=O)Nc1ccc(cc1)C(F)(F)F)c1cccc(c1)C(F)(F)F\n",
      "RDKit ERROR: [14:45:31] SMILES Parse Error: Failed parsing SMILES 'CC(C)(C)OC(=O)N1CCC(CC1)C(=O)Nc1ccc(cc1)C(=O)Nc1ccc(cc1)C(F)(F)F)c1cccc(c1)C(F)(F)F' for input: 'CC(C)(C)OC(=O)N1CCC(CC1)C(=O)Nc1ccc(cc1)C(=O)Nc1ccc(cc1)C(F)(F)F)c1cccc(c1)C(F)(F)F'\n",
      "[14:45:31] SMILES Parse Error: Failed parsing SMILES 'CC(C)(C)OC(=O)N1CCC(CC1)C(=O)Nc1ccc(cc1)C(=O)Nc1ccc(cc1)C(F)(F)F)c1cccc(c1)C(F)(F)F' for input: 'CC(C)(C)OC(=O)N1CCC(CC1)C(=O)Nc1ccc(cc1)C(=O)Nc1ccc(cc1)C(F)(F)F)c1cccc(c1)C(F)(F)F'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail!\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [14:45:33] SMILES Parse Error: extra close parentheses while parsing: c1ccccc1C(c1cccc1)(n1c(c(c(c1)C)C(C)C(=O)O)Cc1ccccc1)c1ccccc1)c1cccccc1\n",
      "[14:45:33] SMILES Parse Error: extra close parentheses while parsing: c1ccccc1C(c1cccc1)(n1c(c(c(c1)C)C(C)C(=O)O)Cc1ccccc1)c1ccccc1)c1cccccc1\n",
      "RDKit ERROR: [14:45:33] SMILES Parse Error: Failed parsing SMILES 'c1ccccc1C(c1cccc1)(n1c(c(c(c1)C)C(C)C(=O)O)Cc1ccccc1)c1ccccc1)c1cccccc1' for input: 'c1ccccc1C(c1cccc1)(n1c(c(c(c1)C)C(C)C(=O)O)Cc1ccccc1)c1ccccc1)c1cccccc1'\n",
      "RDKit ERROR: [14:45:33] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 4 5 6\n",
      "RDKit ERROR: \n",
      "[14:45:33] SMILES Parse Error: Failed parsing SMILES 'c1ccccc1C(c1cccc1)(n1c(c(c(c1)C)C(C)C(=O)O)Cc1ccccc1)c1ccccc1)c1cccccc1' for input: 'c1ccccc1C(c1cccc1)(n1c(c(c(c1)C)C(C)C(=O)O)Cc1ccccc1)c1ccccc1)c1cccccc1'\n",
      "[14:45:33] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 4 5 6\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_51814/2519658259.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrdkit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mchild_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "chemFormer.load_state_dict(torch.load(\"/home/arvid/models/transformers/2021-09-23-12:12:11/weights\"))\n",
    "chemFormer.transformer.dropout = 0.0\n",
    "from rdkit import Chem\n",
    "for i in range(300):\n",
    "    batch = next(iter(dataloaders[\"eval\"]))\n",
    "    src = batch[\"ps\"]\n",
    "    tgt = batch[\"rs\"]\n",
    "    src=src[0,:]\n",
    "\n",
    "    pred = greedy_decode(model=chemFormer, src=src[torch.where(src != 0)].unsqueeze(-1).to(DEVICE),\n",
    "                 src_mask=None, max_len=MAX_SEQ_LEN, start_symbol=1)\n",
    "    \n",
    "    #print(\"\".join([ix_to_char[ix] for ix in tgt[0,:][torch.where(tgt[0,:] != 0)].cpu().numpy()]) + \" >> \" + \"\".join([ix_to_char[ix] for ix in src[torch.where(src != 0)].cpu().numpy()]))\n",
    "    #print(\"prediction: \" + \"\".join([ix_to_char[ix] for ix in pred.squeeze().cpu().numpy()]))\n",
    "    rs = [e for e in \"\".join([ix_to_char[e] for e in pred.squeeze().cpu().numpy()]).split(\".\") if e != \"\"]\n",
    "    ps = [e for e in [\"\".join([ix_to_char[ix] for ix in tgt[0,:][torch.where(tgt[0,:] != 0)].cpu().numpy()])] if e != \"\"]\n",
    "    ps = [e for e in ps[0][1:-1].split(\".\") if e != \"\"]\n",
    "    #print(rs)\n",
    "    #print(ps)\n",
    "    molsp = [Chem.MolFromSmiles(e) for e in rs]\n",
    "    molsg = [Chem.MolFromSmiles(e) for e in ps]\n",
    "    if None in molsp: print(\"Fail!\")\n",
    "    else: \n",
    "        molsp = [Chem.MolToSmiles(e) for e in molsp]\n",
    "        molsg = [Chem.MolToSmiles(e) for e in molsg] \n",
    "        print(molsp)\n",
    "        print(molsg)\n",
    "        if len(molsp) != 0:\n",
    "            truers = 0\n",
    "            for molp in molsp:\n",
    "                for molg in molsg:\n",
    "                    if molp==molg:\n",
    "                        truers +=1\n",
    "                        print(\"truers\")\n",
    "        if truers == len(molp):\n",
    "            print(\"success!\")\n",
    "        else:\n",
    "            print(\"fail!\")\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda107bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = args.lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = [e for e in [\"\".join([ix_to_char[ix] for ix in tgt[0,:][torch.where(tgt[0,:] != 0)].cpu().numpy()])] if e != \"\"]\n",
    "ps = [e for e in ps[0][1:-1].split(\".\") if e != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b776d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print(time.time())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
